# Multimodal_Sentiment_Analysis
æœ¬é¡¹ç›®åŸºäºé…å¯¹çš„æ–‡æœ¬å’Œå›¾åƒï¼Œé¢„æµ‹å¯¹åº”çš„æƒ…æ„Ÿæ ‡ç­¾ï¼ˆpositive, neutral, negativeï¼‰ï¼Œæ¨¡å‹åŒ…æ‹¬ï¼š

- BERT + X (VGG/ResNet/Visual Transformers/BEiT) + MultiheadAttention
- ELECTRA-discriminator + X (VGG/ResNet/Visual Transformers/BEiT) + MultiheadAttention

æœ¬é¡¹ç›®æ˜¯å½“ä»£äººå·¥æ™ºèƒ½è¯¾ç¨‹å®éªŒäº”ä»£ç ä»“åº“ã€‚

## Dataset

æ•°æ®é›†æ¥æºæœªçŸ¥ï¼Œéšå®éªŒå¸ƒç½®å‘æ”¾ï¼Œä¸Šä¼ è‡³ä»“åº“çš„ datasets æ–‡ä»¶å¤¹ä¸‹ã€‚

## Model

æœ¬é¡¹ç›®å®ç°äº†æ–‡æœ¬+å›¾åƒçš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œä¸»è¦å†…å®¹ä¸ºï¼š

- æ–‡æœ¬ç‰¹å¾ï¼š
  - ä½¿ç”¨ BERT æˆ– ELECTRA-discriminator æå–æ–‡æœ¬ç‰¹å¾
  - ä¾æ®ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸åŒï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»– NLP æ¨¡å‹æå–æ–‡æœ¬ç‰¹å¾
- å›¾åƒç‰¹å¾ï¼š
  - VGG16
  - ResNet18 / ResNet50
  - Visual Transformers
  - BEiT
- ç‰¹å¾èåˆï¼š
  - ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†å›¾åƒç‰¹å¾åºåˆ—æˆ–å‘é‡ä½œä¸º query åœ¨æ–‡æœ¬ç‰¹å¾ä¸Šè¿›è¡Œæœç´¢ï¼Œç„¶åä½¿ç”¨å…¨è¿æ¥å±‚è¾“å‡ºåˆ°æ ‡ç­¾ã€‚

## Requirements

æœ¬å®ç°åŸºäº Python3. è¿è¡Œä»£ç éœ€è¦çš„ä¾èµ–åº“å¦‚ä¸‹ï¼š

- torch==1.11.0
- torchvision==0.12.0
- transformers==4.18.0
- numpy==1.21.5
- einops==0.4.1

ä½ å¯ä»¥è¿è¡Œä»¥ä¸‹æŒ‡ä»¤å®‰è£…æ‰€æœ‰ç¯å¢ƒï¼š

```python
pip install -r requirements.txt
```

## Repository structure
éƒ¨åˆ†é‡è¦æ–‡ä»¶æè¿°

```python
|-- datasets # å®Œæ•´æ•°æ®é›†
    |-- data # åŒ…å«äº’ç›¸å¯¹åº”çš„å›¾ç‰‡å’Œæ–‡æœ¬æ•°æ®
    |-- train.txt # è®­ç»ƒé›†æ ·æœ¬çš„ ID å’Œæ ‡ç­¾ã€
    |-- test_without_label.txt # æµ‹è¯•é›†æ ·æœ¬çš„ ID
|-- model # å®éªŒæ¨¡å‹æ–‡ä»¶å¤¹
    |-- MSA.py # å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹
    |-- text_bert.py # æå–æ–‡æœ¬ç‰¹å¾çš„å­æ¨¡å‹
    |-- image_vgg.py # ä½¿ç”¨ VGG æå–å›¾åƒç‰¹å¾çš„å­æ¨¡å‹
    |-- image_resnet.py # ä½¿ç”¨ ResNet æå–å›¾åƒç‰¹å¾çš„å­æ¨¡å‹
    |-- image_vit.py # ä½¿ç”¨ Visual Transformers æå–å›¾åƒç‰¹å¾çš„å­æ¨¡å‹
    |-- image_beit.py # ä½¿ç”¨ BEiT æå–å›¾åƒç‰¹å¾çš„å­æ¨¡å‹
    |-- utils.py # ä¸€äº›è¾…åŠ©æ€§çš„å­æ¨¡å‹
|-- pre-trained-model # é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶å¤¹ï¼Œå¦‚ BERT ç­‰
|-- output # è¾“å‡ºæ–‡ä»¶å¤¹ï¼ŒåŒ…æ‹¬è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ã€é¢„æµ‹ç»“æœç­‰
|-- train.py # æ¨¡å‹è®­ç»ƒè„šæœ¬
|-- test.py # æ¨¡å‹æµ‹è¯•è„šæœ¬
|-- dataset.py # è¯»å–æ•°æ®é›†çš„è„šæœ¬
|-- utils.py # åŒ…å«ä¸€äº›å¸¸ç”¨å‡½æ•°çš„è„šæœ¬
```

## Pretrained Model Required

éœ€è¦ä» ğŸ¤—Huggingface ä¸‹è½½æ¨¡å‹éœ€è¦çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¿å­˜åˆ° pre-trained-model æ–‡ä»¶å¤¹ä¸‹ï¼Œå¦‚ï¼š

- [bert-medium](https://huggingface.co/prajjwal1/bert-medium)
- [bert-base-uncased](https://huggingface.co/bert-base-uncased)
- [electra-small-discriminator](https://huggingface.co/google/electra-small-discriminator)
- [beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)

## Results

å„ä¸ªæ¨¡å‹åœ¨æ•°æ®éªŒè¯é›†ä¸Šçš„ç»“æœï¼ˆå‡†ç¡®ç‡ï¼‰å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

> æ‰€æœ‰æ¨¡å‹çš„ç‰¹å¾èåˆæ¨¡å‹å‡ä¸ºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

| Model | Accuracy |
| ----- | -------- |
| BERT-medium + VGG16 | 70.8%
| BERT-medium + ResNet18 | 71.0%
| BERT-medium + ViT | 69.9%
| BERT-medium + BEiT-base | 71.2%
| ELECTRA-small-discriminator + VGG16 | 64.7%
| ELECTRA-small-discriminator + ResNet18 | 69.0%
| ELECTRA-small-discriminator + ViT | 65.2%
| ELECTRA-small-discriminator + BEiT-base | 67.2%
| BERT-base-uncased + ViT | 71.3%
| BERT-base-uncased + BEiT-base | **72.1%**

- æ¶ˆèå®éªŒ
åˆ†åˆ« mask æ–‡æœ¬æˆ–å›¾åƒï¼ˆè¾“å…¥ç©ºå­—ç¬¦ä¸²æˆ–ç©ºç™½å›¾åƒï¼‰ï¼Œä½¿ç”¨ BERT-base-uncased + BEiT-base æ¨¡å‹åˆ†åˆ«ä»…å¯¹å›¾åƒæˆ–æ–‡æœ¬è¿›è¡Œè®­ç»ƒï¼ŒéªŒè¯é›†å‡†ç¡®ç‡å¦‚ä¸‹ï¼š

| Model | Accuracy |
| ----- | -------- |
| BERT-base-uncased + BEiT-base (mask text) | 57.6%
| BERT-base-uncased + BEiT-base (mask image) | 70.6%


## Quick Start

å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ˆBERT-base-uncased + BEiT-baseï¼‰ï¼Œå¹¶ä¿å­˜ç»“æœ

```shell
python .\train.py 
  --epoch 10 --lr 2e-5 --batch_size 8 --l2 1e-6 
  --scheduler --lr_step 3 --lr_gamma 0.1 
  --model_path .\pre-trained-model\bert-base-uncased\ 
  --image_model beit 
  --image_model_path .\pre-trained-model\beit-base-patch16-224-pt22k-ft22k\ 
  --save_model --cuda

```

## Parameter Setting

è®­ç»ƒä¸»è¦æœ‰ä»¥ä¸‹å‚æ•°ï¼š

- valid_ratioï¼šåœ¨è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†çš„æ¯”ä¾‹
- mask_imageï¼šæ˜¯å¦ mask å›¾ç‰‡ï¼ˆå³è¾“å…¥ç©ºç™½å›¾ç‰‡ï¼‰
- mask_textï¼šæ˜¯å¦ mask æ–‡æœ¬ï¼ˆå³è¾“å…¥ç©ºç™½æ–‡æœ¬ï¼‰
- model_pathï¼šNLP é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
- image_modelï¼šå›¾åƒç‰¹å¾æå–æ¨¡å‹ç±»å‹ï¼Œå¯é€‰çš„åŒ…æ‹¬ vgg, resnet18, resnet50, vit, beit ç­‰
- image_model_pathï¼šå¦‚æœä½¿ç”¨ beitï¼Œå…¶é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„
- epochï¼šè®­ç»ƒè½®æ•°
- batch_sizeï¼šæ‰¹å¤§å°
- lrï¼šå­¦ä¹ ç‡
- schedulerï¼šæ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è®¡åˆ’ï¼ˆStepLRï¼‰
- save_modelï¼šæ˜¯å¦ä¿å­˜æ¨¡å‹å‚æ•°ç­‰è®­ç»ƒç»“æœ
- sizeï¼šå›¾åƒ reshape åçš„å¤§å°
- max_lengthï¼šæ–‡æœ¬é•¿åº¦
- cudaï¼šæ˜¯å¦ä½¿ç”¨ GPU è®­ç»ƒ

## Run pipeline
1. å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¾“å‡ºæµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ

```shell
python .\test.py 
  --save_model_path ./output/2022-07-12-11-43-25 
  --batch_size 8 
  --cuda
```

2. å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤ä½¿ç”¨æŸä¸€ä¸ªå›¾ç‰‡åŠæ–‡æœ¬åšé¢„æµ‹
  
```shell
python .\test.py 
  --pipeline 
  --save_model_path ./output/2022-07-12-11-43-25 
  --image_path .\datasets\data\1.jpg 
  --text_path .\datasets\data\1.txt 
```

## Attribution

éƒ¨åˆ†ä»£ç æ¥æºäºä»¥ä¸‹ä»“åº“/åšå®¢:

- [ViT](https://github.com/tahmid0007/VisualTransformers/blob/main/ResViT.py)

- [æ¨¡å‹å‚æ•°é‡åˆ†æ](https://blog.csdn.net/qq_33757398/article/details/109210240)
